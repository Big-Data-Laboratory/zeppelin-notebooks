{"paragraphs":[{"text":"import org.apache.hadoop.io.Text\nimport org.apache.hadoop.io.LongWritable\n\nval ngrams = sc.sequenceFile(\"s3://emr-workshop-us-west-2/dataset/ngrams/2grams/sequence/\",classOf[LongWritable],classOf[Text]).map(_._2.toString)\n\n// The schema is encoded in a string\nval schemaString = \"gram year occurrences pages books\"\n\n// Import Row.\nimport org.apache.spark.sql.Row;\n\n// Import Spark SQL data types\nimport org.apache.spark.sql.types.{StructType,StructField,StringType};\n\n// Generate the schema based on the string of schema\nval schema =\n  StructType(\n    schemaString.split(\" \").map(fieldName => StructField(fieldName, StringType, true)))\n\n// Convert records of the RDD  to Rows.\nval rowRDD = ngrams.map(_.split('\\t')).map(p => Row(p(0), p(1).trim,p(2).trim,p(3).trim,p(4).trim))\n\n// Apply the schema to the RDD.\nval ngramDataFrame = sqlContext.createDataFrame(rowRDD, schema)\n\n// Register the DataFrames as a table.\nngramDataFrame.registerTempTable(\"2grams\")","dateUpdated":"Jun 19, 2016 1:15:22 AM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466298922399_-1528667716","id":"20160619-011522_1459057682","result":{"code":"SUCCESS","type":"TEXT","msg":"import org.apache.hadoop.io.Text\nimport org.apache.hadoop.io.LongWritable\nngrams: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[1] at map at <console>:32\nschemaString: String = gram year occurrences pages books\nimport org.apache.spark.sql.Row\nimport org.apache.spark.sql.types.{StructType, StructField, StringType}\nschema: org.apache.spark.sql.types.StructType = StructType(StructField(gram,StringType,true), StructField(year,StringType,true), StructField(occurrences,StringType,true), StructField(pages,StringType,true), StructField(books,StringType,true))\nrowRDD: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[3] at map at <console>:37\nngramDataFrame: org.apache.spark.sql.DataFrame = [gram: string, year: string, occurrences: string, pages: string, books: string]\n"},"dateCreated":"Jun 19, 2016 1:15:22 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:869"},{"text":"%sql\nCREATE EXTERNAL TABLE english_2grams_par (\n gram string,\n year int,\n occurrences bigint,\n pages bigint,\n books bigint\n)\nSTORED AS PARQUET\nLOCATION 's3://emr-workshop-us-west-2/dataset/ngrams/2grams/parquet/'","dateUpdated":"Jun 19, 2016 1:19:19 AM","config":{"colWidth":12,"graph":{"mode":"table","height":101,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/sql"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466298922399_-1528667716","id":"20160619-011522_482193236","result":{"code":"SUCCESS","type":"TABLE","msg":"result\n","comment":"","msgTable":[],"columnNames":[{"name":"result","index":0,"aggr":"sum"}],"rows":[]},"dateCreated":"Jun 19, 2016 1:15:22 AM","dateStarted":"Jun 19, 2016 1:17:52 AM","dateFinished":"Jun 19, 2016 1:17:57 AM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1383"},{"title":"Query on Sequnce table","text":"%sql\nselect count(*) from 2grams","dateUpdated":"Jun 19, 2016 1:15:22 AM","config":{"colWidth":6,"graph":{"mode":"table","height":99,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/sql","title":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466298922399_-1528667716","id":"20160619-011522_266696281","result":{"code":"SUCCESS","type":"TABLE","msg":"_c0\n6626604215\n","comment":"","msgTable":[[{"value":"6626604215"}]],"columnNames":[{"name":"_c0","index":0,"aggr":"sum"}],"rows":[["6626604215"]]},"dateCreated":"Jun 19, 2016 1:15:22 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:870","focus":true},{"title":"Query on Parquet table","text":"%sql\nselect count(*) from english_2grams_par limit 10","dateUpdated":"Jun 19, 2016 1:15:22 AM","config":{"colWidth":6,"graph":{"mode":"table","height":104,"optionOpen":false,"keys":[{"name":"_c0","index":0,"aggr":"sum"}],"values":[],"groups":[],"scatter":{"xAxis":{"name":"_c0","index":0,"aggr":"sum"}}},"enabled":true,"editorMode":"ace/mode/sql","title":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466298922399_-1528667716","id":"20160619-011522_566023478","result":{"code":"SUCCESS","type":"TABLE","msg":"_c0\n6626604215\n","comment":"","msgTable":[[{"value":"6626604215"}]],"columnNames":[{"name":"_c0","index":0,"aggr":"sum"}],"rows":[["6626604215"]]},"dateCreated":"Jun 19, 2016 1:15:22 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:871","focus":true},{"text":"%sql\nCREATE TABLE normalized (\n gram string,\n year int,\n occurrences bigint\n)","dateUpdated":"Jun 19, 2016 1:15:22 AM","config":{"colWidth":5,"graph":{"mode":"table","height":98,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/sql"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466298922399_-1528667716","id":"20160619-011522_1951522183","result":{"code":"SUCCESS","type":"TABLE","msg":"result\n","comment":"","msgTable":[],"columnNames":[{"name":"result","index":0,"aggr":"sum"}],"rows":[]},"dateCreated":"Jun 19, 2016 1:15:22 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:872"},{"text":"%sql\n CREATE TABLE by_decade (\n gram string,\n decade int,\n ratio double\n)","dateUpdated":"Jun 19, 2016 1:15:22 AM","config":{"colWidth":6,"graph":{"mode":"table","height":98,"optionOpen":false,"keys":[{"name":"result","index":0,"aggr":"sum"}],"values":[],"groups":[],"scatter":{"xAxis":{"name":"result","index":0,"aggr":"sum"}}},"enabled":true,"editorMode":"ace/mode/sql"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466298922399_-1528667716","id":"20160619-011522_715589050","result":{"code":"SUCCESS","type":"TABLE","msg":"result\n","comment":"","msgTable":[],"columnNames":[{"name":"result","index":0,"aggr":"sum"}],"rows":[]},"dateCreated":"Jun 19, 2016 1:15:22 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:873"},{"text":"sc.hadoopConfiguration.set( \"dfs.blocksize\",\"134217728\" )\nsc.hadoopConfiguration.set( \"parquet.block.size\", \"134217728\" )\nsc.hadoopConfiguration.set(\"parquet.enable.dictionary\",\"true\")\nval normalized = sqlContext.sql(\"SELECT lower(gram) as gram, year, occurrences FROM english_2grams_par WHERE year >= 1890  and gram REGEXP '^[A-Za-z]+ [A-Za-z]*'\")\nnormalized.write.format(\"parquet\").mode(org.apache.spark.sql.SaveMode.Overwrite).saveAsTable(\"normalized\")","dateUpdated":"Jun 19, 2016 1:15:22 AM","config":{"colWidth":12,"graph":{"mode":"table","height":1282.25,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466298922399_-1528667716","id":"20160619-011522_996740150","result":{"code":"SUCCESS","type":"TEXT","msg":"normalized: org.apache.spark.sql.DataFrame = [gram: string, year: int, occurrences: bigint]\n"},"dateCreated":"Jun 19, 2016 1:15:22 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:874"},{"text":"%sql\nshow tables","dateUpdated":"Jun 19, 2016 1:15:22 AM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/sql"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466298922399_-1528667716","id":"20160619-011522_1839254450","result":{"code":"SUCCESS","type":"TABLE","msg":"tableName\tisTemporary\n2grams\ttrue\nby_decade\tfalse\nenglish_2grams\tfalse\nenglish_2grams_par\tfalse\nnormalized\tfalse\nnormalized_sp\tfalse\n","comment":"","msgTable":[[{"key":"isTemporary","value":"2grams"},{"key":"isTemporary","value":"true"}],[{"value":"by_decade"},{"value":"false"}],[{"value":"english_2grams"},{"value":"false"}],[{"value":"english_2grams_par"},{"value":"false"}],[{"value":"normalized"},{"value":"false"}],[{"value":"normalized_sp"},{"value":"false"}]],"columnNames":[{"name":"tableName","index":0,"aggr":"sum"},{"name":"isTemporary","index":1,"aggr":"sum"}],"rows":[["2grams","true"],["by_decade","false"],["english_2grams","false"],["english_2grams_par","false"],["normalized","false"],["normalized_sp","false"]]},"dateCreated":"Jun 19, 2016 1:15:22 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:875"},{"text":"%sql\nselect count(*) from normalized_sp","dateUpdated":"Jun 19, 2016 1:15:22 AM","config":{"colWidth":12,"graph":{"mode":"table","height":129,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/sql"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466298922399_-1528667716","id":"20160619-011522_1469926232","result":{"code":"SUCCESS","type":"TABLE","msg":"_c0\n5467656196\n","comment":"","msgTable":[[{"value":"5467656196"}]],"columnNames":[{"name":"_c0","index":0,"aggr":"sum"}],"rows":[["5467656196"]]},"dateCreated":"Jun 19, 2016 1:15:22 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:876"},{"text":"val by_decade = sqlContext.sql(\"SELECT a.gram, b.decade, sum(a.occurrences)/b.total FROM normalized a JOIN (   SELECT    substr(year, 0, 3) as decade,   sum(occurrences) as total FROM   normalized GROUP BY   substr(year, 0, 3) ) b ON substr(a.year, 0, 3) = b.decade GROUP BY a.gram, b.decade, b.total\")\nby_decade.write.format(\"parquet\").mode(org.apache.spark.sql.SaveMode.Overwrite).saveAsTable(\"by_decade\")\n\n","dateUpdated":"Jun 19, 2016 1:15:22 AM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466298922399_-1528667716","id":"20160619-011522_624700053","result":{"code":"ERROR","type":"TEXT","msg":"by_decade: org.apache.spark.sql.DataFrame = [gram: string, decade: string, _c2: double]\norg.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1.apply$mcV$sp(InsertIntoHadoopFsRelation.scala:156)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1.apply(InsertIntoHadoopFsRelation.scala:108)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1.apply(InsertIntoHadoopFsRelation.scala:108)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:56)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation.run(InsertIntoHadoopFsRelation.scala:108)\n\tat org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult$lzycompute(commands.scala:58)\n\tat org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult(commands.scala:56)\n\tat org.apache.spark.sql.execution.ExecutedCommand.doExecute(commands.scala:70)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$5.apply(SparkPlan.scala:132)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$5.apply(SparkPlan.scala:130)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:130)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:55)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:55)\n\tat org.apache.spark.sql.execution.datasources.ResolvedDataSource$.apply(ResolvedDataSource.scala:256)\n\tat org.apache.spark.sql.hive.execution.CreateMetastoreDataSourceAsSelect.run(commands.scala:274)\n\tat org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult$lzycompute(commands.scala:58)\n\tat org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult(commands.scala:56)\n\tat org.apache.spark.sql.execution.ExecutedCommand.doExecute(commands.scala:70)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$5.apply(SparkPlan.scala:132)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$5.apply(SparkPlan.scala:130)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:130)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:55)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:55)\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:251)\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:221)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:34)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:39)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:41)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:43)\n\tat $iwC$$iwC$$iwC$$iwC.<init>(<console>:45)\n\tat $iwC$$iwC$$iwC.<init>(<console>:47)\n\tat $iwC$$iwC.<init>(<console>:49)\n\tat $iwC.<init>(<console>:51)\n\tat <init>(<console>:53)\n\tat .<init>(<console>:57)\n\tat .<clinit>(<console>)\n\tat .<init>(<console>:7)\n\tat .<clinit>(<console>)\n\tat $print(<console>)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)\n\tat org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1346)\n\tat org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)\n\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)\n\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpretInput(SparkInterpreter.java:709)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:674)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:667)\n\tat org.apache.zeppelin.interpreter.ClassloaderInterpreter.interpret(ClassloaderInterpreter.java:57)\n\tat org.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:93)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:300)\n\tat org.apache.zeppelin.scheduler.Job.run(Job.java:169)\n\tat org.apache.zeppelin.scheduler.FIFOScheduler$1.run(FIFOScheduler.java:134)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.SparkException: Job 32 cancelled part of cancelled job group zeppelin-20160618-042519_944217030\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1370)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply$mcVI$sp(DAGScheduler.scala:783)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply(DAGScheduler.scala:783)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply(DAGScheduler.scala:783)\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobGroupCancelled(DAGScheduler.scala:783)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1619)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1845)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1922)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1.apply$mcV$sp(InsertIntoHadoopFsRelation.scala:150)\n\t... 64 more\n\n"},"dateCreated":"Jun 19, 2016 1:15:22 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1187"},{"text":"%sql \ndesc normalized","dateUpdated":"Jun 19, 2016 1:15:22 AM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/sql"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466298922399_-1528667716","id":"20160619-011522_398306136","result":{"code":"SUCCESS","type":"TABLE","msg":"col_name\tdata_type\tcomment\n_c0\tstring\t\nyear\tint\t\noccurrences\tbigint\t\n","comment":"","msgTable":[[{"key":"data_type","value":"_c0"},{"key":"data_type","value":"string"},{"key":"data_type","value":""}],[{"key":"comment","value":"year"},{"key":"comment","value":"int"},{"key":"comment","value":""}],[{"value":"occurrences"},{"value":"bigint"},{"value":""}]],"columnNames":[{"name":"col_name","index":0,"aggr":"sum"},{"name":"data_type","index":1,"aggr":"sum"},{"name":"comment","index":2,"aggr":"sum"}],"rows":[["_c0","string",""],["year","int",""],["occurrences","bigint",""]]},"dateCreated":"Jun 19, 2016 1:15:22 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:877"},{"text":"normalized.saveAsParquetFile(\"s3://emr-workshop-us-west-2/dataset/ngrams/2grams/normalized/\")\n","dateUpdated":"Jun 19, 2016 1:15:22 AM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466298922399_-1528667716","id":"20160619-011522_286240588","dateCreated":"Jun 19, 2016 1:15:22 AM","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:879"},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466299064871_1435066742","id":"20160619-011744_516354556","dateCreated":"Jun 19, 2016 1:17:44 AM","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1097"}],"name":"Spark Essentials","id":"2BMQV5ERJ","angularObjects":{"2B44YVSN1":[],"2AJXGMUUJ":[],"2AK8P7CPX":[],"2AM1YV5CU":[],"2AKK3QQXU":[],"2ANGGHHMQ":[]},"config":{"looknfeel":"default"},"info":{}}